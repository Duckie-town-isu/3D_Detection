{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accumulate(gt_boxes: EvalBoxes,\n",
    "               pred_boxes: EvalBoxes,\n",
    "               class_name: str,\n",
    "               dist_fcn: Callable,\n",
    "               dist_th: float,\n",
    "               verbose: bool = False) -> DetectionMetricData:\n",
    "    \"\"\"\n",
    "    Average Precision over predefined different recall thresholds for a single distance threshold.\n",
    "    The recall/conf thresholds and other raw metrics will be used in secondary metrics.\n",
    "    :param gt_boxes: Maps every sample_token to a list of its sample_annotations.\n",
    "    :param pred_boxes: Maps every sample_token to a list of its sample_results.\n",
    "    :param class_name: Class to compute AP on.\n",
    "    :param dist_fcn: Distance function used to match detections and ground truths.\n",
    "    :param dist_th: Distance threshold for a match.\n",
    "    :param verbose: If true, print debug messages.\n",
    "    :return: (average_prec, metrics). The average precision value and raw data for a number of metrics.\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------\n",
    "    # Organize input and initialize accumulators.\n",
    "    # ---------------------------------------------\n",
    "    # Count the positives.\n",
    "    npos = len([1 for gt_box in gt_boxes.all if gt_box.detection_name == class_name])\n",
    "    if verbose:\n",
    "        print(\"Found {} GT of class {} out of {} total across {} samples.\".\n",
    "              format(npos, class_name, len(gt_boxes.all), len(gt_boxes.sample_tokens)))\n",
    "\n",
    "    # For missing classes in the GT, return a data structure corresponding to no predictions.\n",
    "    if npos == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # Organize the predictions in a single list.\n",
    "    pred_boxes_list = [box for box in pred_boxes.all if box.detection_name == class_name]\n",
    "    pred_confs = [box.detection_score for box in pred_boxes_list]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Found {} PRED of class {} out of {} total across {} samples.\".\n",
    "              format(len(pred_confs), class_name, len(pred_boxes.all), len(pred_boxes.sample_tokens)))\n",
    "\n",
    "    # Sort by confidence.\n",
    "    sortind = [i for (v, i) in sorted((v, i) for (i, v) in enumerate(pred_confs))][::-1]\n",
    "\n",
    "    # Do the actual matching.\n",
    "    tp = []  # Accumulator of true positives.\n",
    "    fp = []  # Accumulator of false positives.\n",
    "    conf = []  # Accumulator of confidences.\n",
    "\n",
    "    # match_data holds the extra metrics we calculate for each match.\n",
    "    match_data = {'trans_err': [],\n",
    "                  'vel_err': [],\n",
    "                  'scale_err': [],\n",
    "                  'orient_err': [],\n",
    "                  'attr_err': [],\n",
    "                  'conf': []}\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Match and accumulate match data.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    taken = set()  # Initially no gt bounding box is matched.\n",
    "    for ind in sortind:\n",
    "        pred_box = pred_boxes_list[ind]\n",
    "        min_dist = np.inf\n",
    "        match_gt_idx = None\n",
    "\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes[pred_box.sample_token]):\n",
    "\n",
    "            # Find closest match among ground truth boxes\n",
    "            if gt_box.detection_name == class_name and not (pred_box.sample_token, gt_idx) in taken:\n",
    "                this_distance = dist_fcn(gt_box, pred_box)\n",
    "                if this_distance < min_dist:\n",
    "                    min_dist = this_distance\n",
    "                    match_gt_idx = gt_idx\n",
    "\n",
    "        # If the closest match is close enough according to threshold we have a match!\n",
    "        is_match = min_dist < dist_th\n",
    "\n",
    "        if is_match:\n",
    "            taken.add((pred_box.sample_token, match_gt_idx))\n",
    "\n",
    "            #  Update tp, fp and confs.\n",
    "            tp.append(1)\n",
    "            fp.append(0)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "            # Since it is a match, update match data also.\n",
    "            gt_box_match = gt_boxes[pred_box.sample_token][match_gt_idx]\n",
    "\n",
    "            match_data['trans_err'].append(center_distance(gt_box_match, pred_box))\n",
    "            match_data['vel_err'].append(velocity_l2(gt_box_match, pred_box))\n",
    "            match_data['scale_err'].append(1 - scale_iou(gt_box_match, pred_box))\n",
    "\n",
    "            # Barrier orientation is only determined up to 180 degree. (For cones orientation is discarded later)\n",
    "            period = np.pi if class_name == 'barrier' else 2 * np.pi\n",
    "            match_data['orient_err'].append(yaw_diff(gt_box_match, pred_box, period=period))\n",
    "\n",
    "            match_data['attr_err'].append(1 - attr_acc(gt_box_match, pred_box))\n",
    "            match_data['conf'].append(pred_box.detection_score)\n",
    "\n",
    "        else:\n",
    "            # No match. Mark this as a false positive.\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "    # Printing true positive rates of a class:\n",
    "    print(f\"The number of true positives for class {class_name}\")\n",
    "    print(len(tp))\n",
    "\n",
    "    # Check if we have any matches. If not, just return a \"no predictions\" array.\n",
    "    if len(match_data['trans_err']) == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Calculate and interpolate precision and recall\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Accumulate.\n",
    "    tp = np.cumsum(tp).astype(float)\n",
    "    fp = np.cumsum(fp).astype(float)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    # Calculate precision and recall.\n",
    "    prec = tp / (fp + tp)\n",
    "    rec = tp / float(npos)\n",
    "\n",
    "    rec_interp = np.linspace(0, 1, DetectionMetricData.nelem)  # 101 steps, from 0% to 100% recall.\n",
    "    prec = np.interp(rec_interp, rec, prec, right=0)\n",
    "    conf = np.interp(rec_interp, rec, conf, right=0)\n",
    "    rec = rec_interp\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Re-sample the match-data to match, prec, recall and conf.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    for key in match_data.keys():\n",
    "        if key == \"conf\":\n",
    "            continue  # Confidence is used as reference to align with fp and tp. So skip in this step.\n",
    "\n",
    "        else:\n",
    "            # For each match_data, we first calculate the accumulated mean.\n",
    "            tmp = cummean(np.array(match_data[key]))\n",
    "\n",
    "            # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n",
    "            match_data[key] = np.interp(conf[::-1], match_data['conf'][::-1], tmp[::-1])[::-1]\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Done. Instantiate MetricData and return\n",
    "    # ---------------------------------------------\n",
    "    return DetectionMetricData(recall=rec,\n",
    "                               precision=prec,\n",
    "                               confidence=conf,\n",
    "                               trans_err=match_data['trans_err'],\n",
    "                               vel_err=match_data['vel_err'],\n",
    "                               scale_err=match_data['scale_err'],\n",
    "                               orient_err=match_data['orient_err'],\n",
    "                               attr_err=match_data['attr_err'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(gt_boxes: EvalBoxes,\n",
    "               pred_boxes: EvalBoxes,\n",
    "               class_name: str,\n",
    "               dist_fcn: Callable,\n",
    "               dist_th: float,\n",
    "               verbose: bool = False) -> DetectionMetricData:\n",
    "    \"\"\"\n",
    "    Average Precision over predefined different recall thresholds for a single distance threshold.\n",
    "    The recall/conf thresholds and other raw metrics will be used in secondary metrics.\n",
    "    :param gt_boxes: Maps every sample_token to a list of its sample_annotations.\n",
    "    :param pred_boxes: Maps every sample_token to a list of its sample_results.\n",
    "    :param class_name: Class to compute AP on.\n",
    "    :param dist_fcn: Distance function used to match detections and ground truths.\n",
    "    :param dist_th: Distance threshold for a match.\n",
    "    :param verbose: If true, print debug messages.\n",
    "    :return: (average_prec, metrics). The average precision value and raw data for a number of metrics.\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------\n",
    "    # Organize input and initialize accumulators.\n",
    "    # ---------------------------------------------\n",
    "    # Count the positives.\n",
    "    npos = len([1 for gt_box in gt_boxes.all if gt_box.detection_name == class_name])\n",
    "    if verbose:\n",
    "        print(\"Found {} GT of class {} out of {} total across {} samples.\".\n",
    "              format(npos, class_name, len(gt_boxes.all), len(gt_boxes.sample_tokens)))\n",
    "\n",
    "    # For missing classes in the GT, return a data structure corresponding to no predictions.\n",
    "    if npos == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # Organize the predictions in a single list.\n",
    "    pred_boxes_list = [box for box in pred_boxes.all if box.detection_name == class_name]\n",
    "    pred_confs = [box.detection_score for box in pred_boxes_list]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Found {} PRED of class {} out of {} total across {} samples.\".\n",
    "              format(len(pred_confs), class_name, len(pred_boxes.all), len(pred_boxes.sample_tokens)))\n",
    "\n",
    "    # Sort by confidence.\n",
    "    sortind = [i for (v, i) in sorted((v, i) for (i, v) in enumerate(pred_confs))][::-1]\n",
    "\n",
    "    # Do the actual matching.\n",
    "    tp = []  # Accumulator of true positives.\n",
    "    fp = []  # Accumulator of false positives.\n",
    "    conf = []  # Accumulator of confidences.\n",
    "\n",
    "    # match_data holds the extra metrics we calculate for each match.\n",
    "    match_data = {'trans_err': [],\n",
    "                  'vel_err': [],\n",
    "                  'scale_err': [],\n",
    "                  'orient_err': [],\n",
    "                  'attr_err': [],\n",
    "                  'conf': []}\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Match and accumulate match data.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    taken = set()  # Initially no gt bounding box is matched.\n",
    "    for ind in sortind:\n",
    "        pred_box = pred_boxes_list[ind]\n",
    "        min_dist = np.inf\n",
    "        match_gt_idx = None\n",
    "\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes[pred_box.sample_token]):\n",
    "\n",
    "            # Find closest match among ground truth boxes\n",
    "            if gt_box.detection_name == class_name and not (pred_box.sample_token, gt_idx) in taken:\n",
    "                this_distance = dist_fcn(gt_box, pred_box)\n",
    "                if this_distance < min_dist:\n",
    "                    min_dist = this_distance\n",
    "                    match_gt_idx = gt_idx\n",
    "\n",
    "        # If the closest match is close enough according to threshold we have a match!\n",
    "        is_match = min_dist < dist_th\n",
    "\n",
    "        if is_match:\n",
    "            taken.add((pred_box.sample_token, match_gt_idx))\n",
    "\n",
    "            #  Update tp, fp and confs.\n",
    "            tp.append(1)\n",
    "            fp.append(0)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "            # Since it is a match, update match data also.\n",
    "            gt_box_match = gt_boxes[pred_box.sample_token][match_gt_idx]\n",
    "\n",
    "            match_data['trans_err'].append(center_distance(gt_box_match, pred_box))\n",
    "            match_data['vel_err'].append(velocity_l2(gt_box_match, pred_box))\n",
    "            match_data['scale_err'].append(1 - scale_iou(gt_box_match, pred_box))\n",
    "\n",
    "            # Barrier orientation is only determined up to 180 degree. (For cones orientation is discarded later)\n",
    "            period = np.pi if class_name == 'barrier' else 2 * np.pi\n",
    "            match_data['orient_err'].append(yaw_diff(gt_box_match, pred_box, period=period))\n",
    "\n",
    "            match_data['attr_err'].append(1 - attr_acc(gt_box_match, pred_box))\n",
    "            match_data['conf'].append(pred_box.detection_score)\n",
    "\n",
    "        else:\n",
    "            # No match. Mark this as a false positive.\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "    # Check if we have any matches. If not, just return a \"no predictions\" array.\n",
    "    if len(match_data['trans_err']) == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Calculate and interpolate precision and recall\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Accumulate.\n",
    "    tp = np.cumsum(tp).astype(float)\n",
    "    fp = np.cumsum(fp).astype(float)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    # Calculate precision and recall.\n",
    "    prec = tp / (fp + tp)\n",
    "    rec = tp / float(npos)\n",
    "\n",
    "    rec_interp = np.linspace(0, 1, DetectionMetricData.nelem)  # 101 steps, from 0% to 100% recall.\n",
    "    prec = np.interp(rec_interp, rec, prec, right=0)\n",
    "    conf = np.interp(rec_interp, rec, conf, right=0)\n",
    "    rec = rec_interp\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Re-sample the match-data to match, prec, recall and conf.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    for key in match_data.keys():\n",
    "        if key == \"conf\":\n",
    "            continue  # Confidence is used as reference to align with fp and tp. So skip in this step.\n",
    "\n",
    "        else:\n",
    "            # For each match_data, we first calculate the accumulated mean.\n",
    "            tmp = cummean(np.array(match_data[key]))\n",
    "\n",
    "            # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n",
    "            match_data[key] = np.interp(conf[::-1], match_data['conf'][::-1], tmp[::-1])[::-1]\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Done. Instantiate MetricData and return\n",
    "    # ---------------------------------------------\n",
    "    return DetectionMetricData(recall=rec,\n",
    "                               precision=prec,\n",
    "                               confidence=conf,\n",
    "                               trans_err=match_data['trans_err'],\n",
    "                               vel_err=match_data['vel_err'],\n",
    "                               scale_err=match_data['scale_err'],\n",
    "                               orient_err=match_data['orient_err'],\n",
    "                               attr_err=match_data['attr_err'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ap(md: DetectionMetricData, min_recall: float, min_precision: float) -> float:\n",
    "    \"\"\" Calculated average precision. \"\"\"\n",
    "\n",
    "    assert 0 <= min_precision < 1\n",
    "    assert 0 <= min_recall <= 1\n",
    "\n",
    "    prec = np.copy(md.precision)\n",
    "    prec = prec[round(100 * min_recall) + 1:]  # Clip low recalls. +1 to exclude the min recall bin.\n",
    "    prec -= min_precision  # Clip low precision\n",
    "    prec[prec < 0] = 0\n",
    "    return float(np.mean(prec)) / (1.0 - min_precision)\n",
    "\n",
    "def calc_tp(md: DetectionMetricData, min_recall: float, metric_name: str) -> float:\n",
    "    \"\"\" Calculates true positive errors. \"\"\"\n",
    "\n",
    "    first_ind = round(100 * min_recall) + 1  # +1 to exclude the error at min recall.\n",
    "    last_ind = md.max_recall_ind  # First instance of confidence = 0 is index of max achieved recall.\n",
    "    if last_ind < first_ind:\n",
    "        return 1.0  # Assign 1 here. If this happens for all classes, the score for that TP metric will be 0.\n",
    "    else:\n",
    "        return float(np.mean(getattr(md, metric_name)[first_ind: last_ind + 1]))  # +1 to include error at max recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
