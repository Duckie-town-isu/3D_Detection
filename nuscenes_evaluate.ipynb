{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the evaluate.py script of nuscenes\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.eval.common.config import config_factory\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "from nuscenes.eval.common.loaders import load_prediction, load_gt, add_center_dist, filter_eval_boxes\n",
    "# from nuscenes.eval.detection.algo import calc_tp\n",
    "from nuscenes.eval.detection.constants import TP_METRICS\n",
    "from nuscenes.eval.detection.data_classes import DetectionConfig, DetectionMetrics, DetectionBox, \\\n",
    "    DetectionMetricDataList\n",
    "from nuscenes.eval.detection.render import summary_plot, class_pr_curve, class_tp_curve, dist_pr_curve, visualize_sample\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ap(md: DetectionMetricData, min_recall: float, min_precision: float) -> float:\n",
    "    \"\"\" Calculated average precision. \"\"\"\n",
    "\n",
    "    assert 0 <= min_precision < 1\n",
    "    assert 0 <= min_recall <= 1\n",
    "\n",
    "    prec = np.copy(md.precision)\n",
    "    prec = prec[round(100 * min_recall) + 1:]  # Clip low recalls. +1 to exclude the min recall bin.\n",
    "    prec -= min_precision  # Clip low precision\n",
    "    prec[prec < 0] = 0\n",
    "    return float(np.mean(prec)) / (1.0 - min_precision)\n",
    "\n",
    "def calc_tp(md: DetectionMetricData, min_recall: float, metric_name: str) -> float:\n",
    "    \"\"\" Calculates true positive errors. \"\"\"\n",
    "\n",
    "    first_ind = round(100 * min_recall) + 1  # +1 to exclude the error at min recall.\n",
    "    last_ind = md.max_recall_ind  # First instance of confidence = 0 is index of max achieved recall.\n",
    "    if last_ind < first_ind:\n",
    "        return 1.0  # Assign 1 here. If this happens for all classes, the score for that TP metric will be 0.\n",
    "    else:\n",
    "        return float(np.mean(getattr(md, metric_name)[first_ind: last_ind + 1]))  # +1 to include error at max recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accumulate(gt_boxes: EvalBoxes,\n",
    "               pred_boxes: EvalBoxes,\n",
    "               class_name: str,\n",
    "               dist_fcn: Callable,\n",
    "               dist_th: float,\n",
    "               verbose: bool = False) -> DetectionMetricData:\n",
    "    \"\"\"\n",
    "    Average Precision over predefined different recall thresholds for a single distance threshold.\n",
    "    The recall/conf thresholds and other raw metrics will be used in secondary metrics.\n",
    "    :param gt_boxes: Maps every sample_token to a list of its sample_annotations.\n",
    "    :param pred_boxes: Maps every sample_token to a list of its sample_results.\n",
    "    :param class_name: Class to compute AP on.\n",
    "    :param dist_fcn: Distance function used to match detections and ground truths.\n",
    "    :param dist_th: Distance threshold for a match.\n",
    "    :param verbose: If true, print debug messages.\n",
    "    :return: (average_prec, metrics). The average precision value and raw data for a number of metrics.\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------\n",
    "    # Organize input and initialize accumulators.\n",
    "    # ---------------------------------------------\n",
    "    # Count the positives.\n",
    "    print(\"---------------------Custom Accumulate was called-----------------------\")\n",
    "    \n",
    "    # -- npos = This method is returning the number of ground truth annotations for a class \n",
    "    # -- -- gt_boxes is of type EvalBoxes, which groups nusc BBoxes (Translation, Orientation, etc) by sample\n",
    "    # -- -- all returns all bounding boxes\n",
    "    npos = len([1 for gt_box in gt_boxes.all if gt_box.detection_name == class_name])\n",
    "    if verbose:\n",
    "        print(\"Found {} GT of class {} out of {} total across {} samples.\".\n",
    "              format(npos, class_name, len(gt_boxes.all), len(gt_boxes.sample_tokens)))\n",
    "\n",
    "    # For missing classes in the GT, return a data structure corresponding to no predictions.\n",
    "    if npos == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # Organize the predictions in a single list.\n",
    "    pred_boxes_list = [box for box in pred_boxes.all if box.detection_name == class_name]\n",
    "    pred_confs = [box.detection_score for box in pred_boxes_list]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Found {} PRED of class {} out of {} total across {} samples.\".\n",
    "              format(len(pred_confs), class_name, len(pred_boxes.all), len(pred_boxes.sample_tokens)))\n",
    "\n",
    "    # Sort by confidence.\n",
    "    sortind = [i for (v, i) in sorted((v, i) for (i, v) in enumerate(pred_confs))][::-1]\n",
    "\n",
    "    # Do the actual matching.\n",
    "    tp = []  # Accumulator of true positives.\n",
    "    fp = []  # Accumulator of false positives.\n",
    "    conf = []  # Accumulator of confidences.\n",
    "\n",
    "    # match_data holds the extra metrics we calculate for each match.\n",
    "    match_data = {'trans_err': [],\n",
    "                  'vel_err': [],\n",
    "                  'scale_err': [],\n",
    "                  'orient_err': [],\n",
    "                  'attr_err': [],\n",
    "                  'conf': []}\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Match and accumulate match data.\n",
    "    # ---------------------------------------------\n",
    "    # -- Get the prediction with the greatest confidence\n",
    "    # -- -- pred_boxes_list is a list of BBoxes and match it with no gt\n",
    "    # -- -- for each ground truth bounding box\n",
    "    # -- -- -- if (the ground truth bounding box is belongs to the same class AND hasn't been assigned yet)\n",
    "    # -- -- -- -- consider this for a possible match. \n",
    "    #             Exit the loop with the a same class match that is the least distance away\n",
    "    # \n",
    "    taken = set()  # Initially no gt bounding box is matched.\n",
    "    for ind in sortind:\n",
    "        pred_box = pred_boxes_list[ind]\n",
    "        min_dist = np.inf\n",
    "        match_gt_idx = None\n",
    "\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes[pred_box.sample_token]):\n",
    "\n",
    "            # Find closest match among ground truth boxes\n",
    "            if gt_box.detection_name == class_name and not (pred_box.sample_token, gt_idx) in taken:\n",
    "                this_distance = dist_fcn(gt_box, pred_box)\n",
    "                if this_distance < min_dist:\n",
    "                    min_dist = this_distance\n",
    "                    match_gt_idx = gt_idx\n",
    "\n",
    "        # If the closest match is close enough according to threshold we have a match!\n",
    "        is_match = min_dist < dist_th\n",
    "\n",
    "        if is_match:\n",
    "            taken.add((pred_box.sample_token, match_gt_idx))\n",
    "\n",
    "            #  Update tp, fp and confs.\n",
    "            tp.append(1)\n",
    "            fp.append(0)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "            # Since it is a match, update match data also.\n",
    "            gt_box_match = gt_boxes[pred_box.sample_token][match_gt_idx]\n",
    "\n",
    "            match_data['trans_err'].append(center_distance(gt_box_match, pred_box))\n",
    "            match_data['vel_err'].append(velocity_l2(gt_box_match, pred_box))\n",
    "            match_data['scale_err'].append(1 - scale_iou(gt_box_match, pred_box))\n",
    "\n",
    "            # Barrier orientation is only determined up to 180 degree. (For cones orientation is discarded later)\n",
    "            period = np.pi if class_name == 'barrier' else 2 * np.pi\n",
    "            match_data['orient_err'].append(yaw_diff(gt_box_match, pred_box, period=period))\n",
    "\n",
    "            match_data['attr_err'].append(1 - attr_acc(gt_box_match, pred_box))\n",
    "            match_data['conf'].append(pred_box.detection_score)\n",
    "\n",
    "        else:\n",
    "            # No match. Mark this as a false positive.\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "    # Printing true positive rates of a class:\n",
    "    print(f\"The number of true positives out of total instances for class {class_name} for {dist_th}m \\n {len(tp)} out of {npos} \\n\")\n",
    "\n",
    "    # Check if we have any matches. If not, just return a \"no predictions\" array.\n",
    "    if len(match_data['trans_err']) == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Calculate and interpolate precision and recall\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Accumulate.\n",
    "    tp = np.cumsum(tp).astype(float)\n",
    "    fp = np.cumsum(fp).astype(float)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    # Calculate precision and recall.\n",
    "    prec = tp / (fp + tp)\n",
    "    rec = tp / float(npos)\n",
    "\n",
    "    rec_interp = np.linspace(0, 1, DetectionMetricData.nelem)  # 101 steps, from 0% to 100% recall.\n",
    "    prec = np.interp(rec_interp, rec, prec, right=0)\n",
    "    conf = np.interp(rec_interp, rec, conf, right=0)\n",
    "    rec = rec_interp\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Re-sample the match-data to match, prec, recall and conf.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    for key in match_data.keys():\n",
    "        if key == \"conf\":\n",
    "            continue  # Confidence is used as reference to align with fp and tp. So skip in this step.\n",
    "\n",
    "        else:\n",
    "            # For each match_data, we first calculate the accumulated mean.\n",
    "            tmp = cummean(np.array(match_data[key]))\n",
    "\n",
    "            # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n",
    "            match_data[key] = np.interp(conf[::-1], match_data['conf'][::-1], tmp[::-1])[::-1]\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Done. Instantiate MetricData and return\n",
    "    # ---------------------------------------------\n",
    "    return DetectionMetricData(recall=rec,\n",
    "                               precision=prec,\n",
    "                               confidence=conf,\n",
    "                               trans_err=match_data['trans_err'],\n",
    "                               vel_err=match_data['vel_err'],\n",
    "                               scale_err=match_data['scale_err'],\n",
    "                               orient_err=match_data['orient_err'],\n",
    "                               attr_err=match_data['attr_err'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionEval:\n",
    "    \"\"\"\n",
    "    This is the official nuScenes detection evaluation code.\n",
    "    Results are written to the provided output_dir.\n",
    "\n",
    "    nuScenes uses the following detection metrics:\n",
    "    - Mean Average Precision (mAP): Uses center-distance as matching criterion; averaged over distance thresholds.\n",
    "    - True Positive (TP) metrics: Average of translation, velocity, scale, orientation and attribute errors.\n",
    "    - nuScenes Detection Score (NDS): The weighted sum of the above.\n",
    "\n",
    "    Here is an overview of the functions in this method:\n",
    "    - init: Loads GT annotations and predictions stored in JSON format and filters the boxes.\n",
    "    - run: Performs evaluation and dumps the metric data to disk.\n",
    "    - render: Renders various plots and dumps to disk.\n",
    "\n",
    "    We assume that:\n",
    "    - Every sample_token is given in the results, although there may be not predictions for that sample.\n",
    "\n",
    "    Please see https://www.nuscenes.org/object-detection for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nusc: NuScenes,\n",
    "                 config: DetectionConfig,\n",
    "                 result_path: str,\n",
    "                 eval_set: str,\n",
    "                 output_dir: str = None,\n",
    "                 verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize a DetectionEval object.\n",
    "        :param nusc: A NuScenes object.\n",
    "        :param config: A DetectionConfig object.\n",
    "        :param result_path: Path of the nuScenes JSON result file.\n",
    "        :param eval_set: The dataset split to evaluate on, e.g. train, val or test.\n",
    "        :param output_dir: Folder to save plots and results to.\n",
    "        :param verbose: Whether to print to stdout.\n",
    "        \"\"\"\n",
    "        self.nusc = nusc\n",
    "        self.result_path = result_path\n",
    "        self.eval_set = eval_set\n",
    "        self.output_dir = output_dir\n",
    "        self.verbose = verbose\n",
    "        self.cfg = config\n",
    "\n",
    "        # Check result file exists.\n",
    "        assert os.path.exists(result_path), 'Error: The result file does not exist!'\n",
    "\n",
    "        # Make dirs.\n",
    "        self.plot_dir = os.path.join(self.output_dir, 'plots')\n",
    "        if not os.path.isdir(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        if not os.path.isdir(self.plot_dir):\n",
    "            os.makedirs(self.plot_dir)\n",
    "\n",
    "        # Load data.\n",
    "        if verbose:\n",
    "            print('Initializing nuScenes detection evaluation')\n",
    "        self.pred_boxes, self.meta = load_prediction(self.result_path, self.cfg.max_boxes_per_sample, DetectionBox,\n",
    "                                                     verbose=verbose)\n",
    "        self.gt_boxes = load_gt(self.nusc, self.eval_set, DetectionBox, verbose=verbose)\n",
    "\n",
    "        assert set(self.pred_boxes.sample_tokens) == set(self.gt_boxes.sample_tokens), \\\n",
    "            \"Samples in split doesn't match samples in predictions.\"\n",
    "\n",
    "        # Add center distances.\n",
    "        self.pred_boxes = add_center_dist(nusc, self.pred_boxes)\n",
    "        self.gt_boxes = add_center_dist(nusc, self.gt_boxes)\n",
    "\n",
    "        # Filter boxes (distance, points per box, etc.).\n",
    "        if verbose:\n",
    "            print('Filtering predictions')\n",
    "        self.pred_boxes = filter_eval_boxes(nusc, self.pred_boxes, self.cfg.class_range, verbose=verbose)\n",
    "        if verbose:\n",
    "            print('Filtering ground truth annotations')\n",
    "        self.gt_boxes = filter_eval_boxes(nusc, self.gt_boxes, self.cfg.class_range, verbose=verbose)\n",
    "\n",
    "        self.sample_tokens = self.gt_boxes.sample_tokens\n",
    "\n",
    "    def custom_evaluate(self) -> Tuple[DetectionMetrics, DetectionMetricDataList]:\n",
    "        \"\"\"\n",
    "        Performs the actual evaluation, but customized for metrics that are\n",
    "        of interest to us.\n",
    "        :return: A tuple of high-level and the raw metric data.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 1: Accumulate metric data for all classes and distance thresholds.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Accumulating metric data...')\n",
    "        metric_data_list = DetectionMetricDataList()\n",
    "        \n",
    "        # -- For each class\n",
    "        # -- -- For each distance threshold of this class\n",
    "        # -- -- -- return the metric data for this class\n",
    "        # Q: Shouldn't the number of npos also change as dist_th changes?\n",
    "        for class_name in self.cfg.class_names:\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                md = custom_accumulate(self.gt_boxes, self.pred_boxes, class_name, self.cfg.dist_fcn_callable, dist_th)\n",
    "                metric_data_list.set(class_name, dist_th, md)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 2: Calculate metrics from the data.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Calculating metrics...')\n",
    "        metrics = DetectionMetrics(self.cfg)\n",
    "        for class_name in self.cfg.class_names:\n",
    "            # Compute APs.\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                metric_data = metric_data_list[(class_name, dist_th)]\n",
    "                ap = calc_ap(metric_data, self.cfg.min_recall, self.cfg.min_precision)\n",
    "                metrics.add_label_ap(class_name, dist_th, ap)\n",
    "\n",
    "            # Compute TP metrics.\n",
    "            for metric_name in TP_METRICS:\n",
    "                metric_data = metric_data_list[(class_name, self.cfg.dist_th_tp)]\n",
    "                if class_name in ['traffic_cone'] and metric_name in ['attr_err', 'vel_err', 'orient_err']:\n",
    "                    tp = np.nan\n",
    "                elif class_name in ['barrier'] and metric_name in ['attr_err', 'vel_err']:\n",
    "                    tp = np.nan\n",
    "                else:\n",
    "                    tp = calc_tp(metric_data, self.cfg.min_recall, metric_name)\n",
    "                metrics.add_label_tp(class_name, metric_name, tp)\n",
    "\n",
    "        # Compute evaluation time.\n",
    "        metrics.add_runtime(time.time() - start_time)\n",
    "\n",
    "        return metrics, metric_data_list\n",
    "    \n",
    "    from nuscenes.eval.detection.algo import accumulate\n",
    "    def evaluate(self) -> Tuple[DetectionMetrics, DetectionMetricDataList]:\n",
    "        \"\"\"\n",
    "        Performs the actual evaluation.\n",
    "        :return: A tuple of high-level and the raw metric data.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 1: Accumulate metric data for all classes and distance thresholds.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Accumulating metric data...')\n",
    "        metric_data_list = DetectionMetricDataList()\n",
    "        for class_name in self.cfg.class_names:\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                md = accumulate(self.gt_boxes, self.pred_boxes, class_name, self.cfg.dist_fcn_callable, dist_th)\n",
    "                metric_data_list.set(class_name, dist_th, md)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 2: Calculate metrics from the data.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Calculating metrics...')\n",
    "        metrics = DetectionMetrics(self.cfg)\n",
    "        for class_name in self.cfg.class_names:\n",
    "            # Compute APs.\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                metric_data = metric_data_list[(class_name, dist_th)]\n",
    "                ap = calc_ap(metric_data, self.cfg.min_recall, self.cfg.min_precision)\n",
    "                metrics.add_label_ap(class_name, dist_th, ap)\n",
    "\n",
    "            # Compute TP metrics.\n",
    "            for metric_name in TP_METRICS:\n",
    "                metric_data = metric_data_list[(class_name, self.cfg.dist_th_tp)]\n",
    "                if class_name in ['traffic_cone'] and metric_name in ['attr_err', 'vel_err', 'orient_err']:\n",
    "                    tp = np.nan\n",
    "                elif class_name in ['barrier'] and metric_name in ['attr_err', 'vel_err']:\n",
    "                    tp = np.nan\n",
    "                else:\n",
    "                    tp = calc_tp(metric_data, self.cfg.min_recall, metric_name)\n",
    "                metrics.add_label_tp(class_name, metric_name, tp)\n",
    "\n",
    "        # Compute evaluation time.\n",
    "        metrics.add_runtime(time.time() - start_time)\n",
    "\n",
    "        return metrics, metric_data_list\n",
    "\n",
    "    def render(self, metrics: DetectionMetrics, md_list: DetectionMetricDataList) -> None:\n",
    "        \"\"\"\n",
    "        Renders various PR and TP curves.\n",
    "        :param metrics: DetectionMetrics instance.\n",
    "        :param md_list: DetectionMetricDataList instance.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print('Rendering PR and TP curves')\n",
    "\n",
    "        def savepath(name):\n",
    "            return os.path.join(self.plot_dir, name + '.pdf')\n",
    "\n",
    "        summary_plot(md_list, metrics, min_precision=self.cfg.min_precision, min_recall=self.cfg.min_recall,\n",
    "                     dist_th_tp=self.cfg.dist_th_tp, savepath=savepath('summary'))\n",
    "\n",
    "        for detection_name in self.cfg.class_names:\n",
    "            class_pr_curve(md_list, metrics, detection_name, self.cfg.min_precision, self.cfg.min_recall,\n",
    "                           savepath=savepath(detection_name + '_pr'))\n",
    "\n",
    "            class_tp_curve(md_list, metrics, detection_name, self.cfg.min_recall, self.cfg.dist_th_tp,\n",
    "                           savepath=savepath(detection_name + '_tp'))\n",
    "\n",
    "        for dist_th in self.cfg.dist_ths:\n",
    "            dist_pr_curve(md_list, metrics, dist_th, self.cfg.min_precision, self.cfg.min_recall,\n",
    "                          savepath=savepath('dist_pr_' + str(dist_th)))\n",
    "\n",
    "    def main(self,\n",
    "             plot_examples: int = 0,\n",
    "             render_curves: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main function that loads the evaluation code, visualizes samples, runs the evaluation and renders stat plots.\n",
    "        :param plot_examples: How many example visualizations to write to disk.\n",
    "        :param render_curves: Whether to render PR and TP curves to disk.\n",
    "        :return: A dict that stores the high-level metrics and meta data.\n",
    "        \"\"\"\n",
    "        if plot_examples > 0:\n",
    "            # Select a random but fixed subset to plot.\n",
    "            random.seed(42)\n",
    "            sample_tokens = list(self.sample_tokens)\n",
    "            random.shuffle(sample_tokens)\n",
    "            sample_tokens = sample_tokens[:plot_examples]\n",
    "\n",
    "            # Visualize samples.\n",
    "            example_dir = os.path.join(self.output_dir, 'examples')\n",
    "            if not os.path.isdir(example_dir):\n",
    "                os.mkdir(example_dir)\n",
    "            for sample_token in sample_tokens:\n",
    "                visualize_sample(self.nusc,\n",
    "                                 sample_token,\n",
    "                                 self.gt_boxes if self.eval_set != 'test' else EvalBoxes(),\n",
    "                                 # Don't render test GT.\n",
    "                                 self.pred_boxes,\n",
    "                                 eval_range=max(self.cfg.class_range.values()),\n",
    "                                 savepath=os.path.join(example_dir, '{}.png'.format(sample_token)))\n",
    "\n",
    "        # Run evaluation.\n",
    "        metrics, metric_data_list = self.evaluate()\n",
    "\n",
    "        # Render PR and TP curves.\n",
    "        if render_curves:\n",
    "            self.render(metrics, metric_data_list)\n",
    "\n",
    "        # Dump the metric data, meta and metrics to disk.\n",
    "        if self.verbose:\n",
    "            print('Saving metrics to: %s' % self.output_dir)\n",
    "        metrics_summary = metrics.serialize()\n",
    "        metrics_summary['meta'] = self.meta.copy()\n",
    "        with open(os.path.join(self.output_dir, 'metrics_summary.json'), 'w') as f:\n",
    "            json.dump(metrics_summary, f, indent=2)\n",
    "        with open(os.path.join(self.output_dir, 'metrics_details.json'), 'w') as f:\n",
    "            json.dump(metric_data_list.serialize(), f, indent=2)\n",
    "\n",
    "        # Print high-level metrics.\n",
    "        print('mAP: %.4f' % (metrics_summary['mean_ap']))\n",
    "        err_name_mapping = {\n",
    "            'trans_err': 'mATE',\n",
    "            'scale_err': 'mASE',\n",
    "            'orient_err': 'mAOE',\n",
    "            'vel_err': 'mAVE',\n",
    "            'attr_err': 'mAAE'\n",
    "        }\n",
    "        for tp_name, tp_val in metrics_summary['tp_errors'].items():\n",
    "            print('%s: %.4f' % (err_name_mapping[tp_name], tp_val))\n",
    "        print('NDS: %.4f' % (metrics_summary['nd_score']))\n",
    "        print('Eval time: %.1fs' % metrics_summary['eval_time'])\n",
    "\n",
    "        # Print per-class metrics.\n",
    "        print()\n",
    "        print('Per-class results:')\n",
    "        print('%-20s\\t%-6s\\t%-6s\\t%-6s\\t%-6s\\t%-6s\\t%-6s' % ('Object Class', 'AP', 'ATE', 'ASE', 'AOE', 'AVE', 'AAE'))\n",
    "        class_aps = metrics_summary['mean_dist_aps']\n",
    "        class_tps = metrics_summary['label_tp_errors']\n",
    "        for class_name in class_aps.keys():\n",
    "            print('%-20s\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f'\n",
    "                % (class_name, class_aps[class_name],\n",
    "                    class_tps[class_name]['trans_err'],\n",
    "                    class_tps[class_name]['scale_err'],\n",
    "                    class_tps[class_name]['orient_err'],\n",
    "                    class_tps[class_name]['vel_err'],\n",
    "                    class_tps[class_name]['attr_err']))\n",
    "\n",
    "        return metrics_summary\n",
    "\n",
    "\n",
    "class NuScenesEval(DetectionEval):\n",
    "    \"\"\"\n",
    "    Dummy class for backward-compatibility. Same as DetectionEval.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import cls_attr_dist, class_names, mini_val_tokens\n",
    "from custom_env import home_dir, output_dir, preds_dir, model_dir, is_set_to_mini\n",
    "from custom_env import dataset_root as dataroot\n",
    "\n",
    "from mmdet3d.evaluation.metrics import nuscenes_metric as nus_metric\n",
    "\n",
    "import import_ipynb\n",
    "import nuscenes_accumulate\n",
    "import nuscenes_evaluate\n",
    "\n",
    "eval_set_map = {\n",
    "        'v1.0-mini': 'mini_val',\n",
    "        'v1.0-trainval': 'val',\n",
    "        'v1.0-test': 'test'\n",
    "    }\n",
    "\n",
    "dataset_version = 'v1.0-mini' if is_set_to_mini() else 'v1.0-trainval'\n",
    "try:\n",
    "    eval_version = 'detection_cvpr_2019'\n",
    "    eval_config = config_factory(eval_version)\n",
    "except:\n",
    "    eval_version = 'cvpr_2019'\n",
    "    eval_config = config_factory(eval_version)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD = 0.4\n",
    "\n",
    "backend_args = None\n",
    "# nusc = NuScenes(version='v1.0-trainval', dataroot = f\"{home_dir}/software/mmdetection3d/data/nuscenes\")\n",
    "nusc = NuScenes(version=dataset_version, dataroot = f\"{home_dir}/software/mmdetection3d/data/nuscenes-mini\")\n",
    "ann_file=dataroot + 'nuscenes_infos_val.pkl'\n",
    "metric='bbox'\n",
    "\n",
    "pcd_path = f\"{home_dir}/software/mmdetection3d/data/nuscenes-mini/samples/LIDAR_TOP/\"\n",
    "mmdet_path = f\"{home_dir}/software/mmdetection3d\"\n",
    "pcd_list = os.listdir(pcd_path)\n",
    "\n",
    "# Instantiate evaluator:\n",
    "evaluator = nus_metric.NuScenesMetric(dataroot, ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomDetectionEval = DetectionEval\n",
    "\n",
    "eval = CustomDetectionEval(\n",
    "    nusc=nusc,\n",
    "    config=eval_config,\n",
    "    result_path=f'{model_dir}/results_nusc.json',\n",
    "    eval_set=eval_set_map[dataset_version],\n",
    "    output_dir=os.getcwd(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.custom_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q: Shouldn't the number of npos also change as dist_th changes?***\n",
    "\n",
    "    Seems like the number of false positives is a little inflated\n",
    "    Currently the code functions as such\n",
    "\n",
    "    ```\n",
    "    evaluate()\n",
    "        for each class\n",
    "            for one of 4 dist_thresholds in  [0.5, 1, 2, 4]m\n",
    "                def accumulate(gt, pred, dist thresh)\n",
    "                    if we find a successful match and it is less than the dist_thresh count that as tp\n",
    "                    else consider it a false positive\n",
    "                    NOTE: Look at the comments in that method before reading further\n",
    "    ```\n",
    "\n",
    "This is why you'll see that the values don't change for the different thresholds. I would think npos should change as we change what dist threshold to consider but it seems like it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
