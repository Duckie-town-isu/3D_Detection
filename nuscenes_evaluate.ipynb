{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the evaluate.py script of nuscenes\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pytorch3d\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from pyquaternion import Quaternion\n",
    "from itertools import chain, combinations\n",
    "\n",
    "\n",
    "from custom_env import dataset_root as dataroot\n",
    "from classes import cls_attr_dist, class_names, mini_val_tokens\n",
    "from mmdet3d.evaluation.metrics import nuscenes_metric as nus_metric\n",
    "from custom_env import home_dir, output_dir, preds_dir, model_dir, is_set_to_mini\n",
    "\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.eval.common.config import config_factory\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "from nuscenes.eval.detection.constants import TP_METRICS\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes, EvalBox\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData\n",
    "from nuscenes.eval.common.loaders import load_prediction, load_gt, add_center_dist, filter_eval_boxes\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "from nuscenes.eval.detection.render import summary_plot, class_pr_curve, class_tp_curve, dist_pr_curve, visualize_sample\n",
    "from nuscenes.eval.detection.data_classes import DetectionConfig, DetectionMetrics, DetectionBox, DetectionMetricDataList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set_map = {\n",
    "        'v1.0-mini': 'mini_val',\n",
    "        'v1.0-trainval': 'val',\n",
    "        'v1.0-test': 'test'\n",
    "    }\n",
    "\n",
    "dataset_version = 'v1.0-mini' if is_set_to_mini() else 'v1.0-trainval'\n",
    "try:\n",
    "    eval_version = 'detection_cvpr_2019'\n",
    "    eval_config = config_factory(eval_version)\n",
    "except:\n",
    "    eval_version = 'cvpr_2019'\n",
    "    eval_config = config_factory(eval_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.564 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "DETECTION_THRESHOLD = 0.35\n",
    "\n",
    "backend_args = None\n",
    "nusc = NuScenes(version=dataset_version, dataroot = dataroot)\n",
    "ann_file = f'{dataroot}nuscenes_infos_val.pkl'\n",
    "metric='bbox'\n",
    "\n",
    "pcd_path = f\"{dataroot}/samples/LIDAR_TOP/\"\n",
    "mmdet_path = f\"{home_dir}/software/mmdetection3d\"\n",
    "pcd_list = os.listdir(pcd_path)\n",
    "\n",
    "# Instantiate evaluator:\n",
    "evaluator = nus_metric.NuScenesMetric(dataroot, ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "PED = 0\n",
    "OBS = 1\n",
    "EMPTY = 2\n",
    "\n",
    "distance_param_conf_mat = np.zeros((3, 3))\n",
    "\n",
    "conf_mat_mapping = {\n",
    "    \"pedestrian\": PED,\n",
    "    \"bus\": OBS,\n",
    "    \"car\" : OBS,\n",
    "    \"truck\": OBS,\n",
    "    \"bicycle\": OBS,\n",
    "    \"motorcycle\": OBS,\n",
    "    \"traffic_cone\": OBS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_evalbox_to_box(input_list:EvalBoxes) -> List[Box]:\n",
    "    boxes_out = []\n",
    "    for box in input_list.all:\n",
    "        # Create Box instance.\n",
    "        box = Box(box.translation, box.size, Quaternion(box.rotation), name=box.detection_name, token=box.sample_token, score=box.detection_score)\n",
    "        boxes_out.append(box)\n",
    "    return boxes_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_param_conf_mat(gt_boxes:EvalBoxes, \n",
    "                                      pred_boxes: EvalBoxes, \n",
    "                                      sortind, \n",
    "                                      class_name:str,\n",
    "                                      tp:list, \n",
    "                                      fp:list, \n",
    "                                      conf:list,\n",
    "                                      taken:list, \n",
    "                                      match_data:DetectionMetricData,\n",
    "                                      conf_mat_mapping: Dict,\n",
    "                                      dist_thresh: float = 2.0,       # in m \n",
    "                                      yaw_thresh: float = np.pi/2.0): # in radians  -> np.ndarray:\n",
    "        \n",
    "        converted_gt = convert_evalbox_to_box(gt_boxes)\n",
    "        corners = np.array([box.corners() for box in converted_gt])\n",
    "        tokens = np.array([box.token for box in converted_gt])\n",
    "        _ = 4\n",
    "        \n",
    "        # -- For each sample\n",
    "        # -- -- For each ground truth\n",
    "        # -- -- -- For each prediction\n",
    "        # -- -- -- -- If pred meets match criteria and not already matched, add to matches.\n",
    "        # -- -- -- For all the matches matches, pick the one with highest score.\n",
    "        for sample_token in gt_boxes.sample_tokens:\n",
    "                sample_pred_list = pred_boxes[sample_token]\n",
    "                sample_gt_list = gt_boxes[sample_token]\n",
    "                taken = set()  # Initially no gt bounding box is matched.\n",
    "                \n",
    "                # check if there are phantom predictions\n",
    "                class_pred_len = [len([1 for pred in sample_pred_list if pred.detection_name == class_name]) for class_name in conf_mat_mapping]\n",
    "                class_gt_len = [len([1 for gt in sample_gt_list if gt.detection_name == class_name]) for class_name in conf_mat_mapping]\n",
    "                \n",
    "                for i in range(len(class_pred_len)):\n",
    "                        if class_pred_len[i] > class_gt_len[i]:\n",
    "                                distance_param_conf_mat[i][EMPTY] += class_pred_len[i] - class_gt_len[i]\n",
    "                \n",
    "                for gt in sample_gt_list:\n",
    "                        \n",
    "                        best_iou = -1       # Initialize best iou for a bbox with a value that cannot be achieved.\n",
    "                        best_match = None   # Initialize best matching bbox with None. Tuple of (gt, pred, iou)\n",
    "                        match_pred_ids = [] # Initialize list of matched predictions for this gt.\n",
    "                        \n",
    "                        for i, pred in enumerate(sample_pred_list):\n",
    "                                if center_distance(pred, gt) < dist_thresh and yaw_diff(pred, gt) < yaw_thresh and i not in taken:\n",
    "                                        match_pred_ids.append(i)\n",
    "                                        \n",
    "                        for match_idx in match_pred_ids:\n",
    "                                iou = scale_iou(sample_pred_list[match_idx], gt)\n",
    "                                if best_iou < iou:\n",
    "                                        best_iou = iou\n",
    "                                        best_match = (gt, sample_pred_list[match_idx], match_idx)\n",
    "                        \n",
    "                        if len(match_pred_ids) == 0:\n",
    "                                distance_param_conf_mat[conf_mat_mapping[gt.detection_name]][EMPTY] += 1\n",
    "                                continue\n",
    "                        else:\n",
    "                                taken.add(best_match[2])\n",
    "                                distance_param_conf_mat[conf_mat_mapping[best_match[0].detection_name]][conf_mat_mapping[best_match[1].detection_name]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prop_labelled_conf_mat (gt_boxes:EvalBoxes, \n",
    "                                      pred_boxes_list: list, \n",
    "                                      sortind,\n",
    "                                      list_of_propositions: list, \n",
    "                                      class_name:str, \n",
    "                                      tp:list, \n",
    "                                      fp:list, \n",
    "                                      conf:list,\n",
    "                                      taken:list, \n",
    "                                      match_data:DetectionMetricData) -> np.ndarray:\n",
    "    \n",
    "    propn_indices = range(len(list_of_propositions))\n",
    "    propn_powerset = powerset(propn_indices)\n",
    "    \n",
    "    for propn in propn_powerset:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ap(md: DetectionMetricData, min_recall: float, min_precision: float) -> float:\n",
    "    \"\"\" Calculated average precision. \"\"\"\n",
    "\n",
    "    assert 0 <= min_precision < 1\n",
    "    assert 0 <= min_recall <= 1\n",
    "\n",
    "    prec = np.copy(md.precision)\n",
    "    prec = prec[round(100 * min_recall) + 1:]  # Clip low recalls. +1 to exclude the min recall bin.\n",
    "    prec -= min_precision  # Clip low precision\n",
    "    prec[prec < 0] = 0\n",
    "    return float(np.mean(prec)) / (1.0 - min_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tp(md: DetectionMetricData, min_recall: float, metric_name: str) -> float:\n",
    "    \"\"\" Calculates true positive errors. \"\"\"\n",
    "\n",
    "    first_ind = round(100 * min_recall) + 1  # +1 to exclude the error at min recall.\n",
    "    last_ind = md.max_recall_ind  # First instance of confidence = 0 is index of max achieved recall.\n",
    "    if last_ind < first_ind:\n",
    "        return 1.0  # Assign 1 here. If this happens for all classes, the score for that TP metric will be 0.\n",
    "    else:\n",
    "        return float(np.mean(getattr(md, metric_name)[first_ind: last_ind + 1]))  # +1 to include error at max recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accumulate(gt_boxes: EvalBoxes,\n",
    "               pred_boxes: EvalBoxes,\n",
    "               class_name: str,\n",
    "               dist_fcn: Callable,\n",
    "               dist_th: float,\n",
    "               verbose: bool = False) -> DetectionMetricData:\n",
    "    \"\"\"\n",
    "    Average Precision over predefined different recall thresholds for a single distance threshold.\n",
    "    The recall/conf thresholds and other raw metrics will be used in secondary metrics.\n",
    "    :param gt_boxes: Maps every sample_token to a list of its sample_annotations.\n",
    "    :param pred_boxes: Maps every sample_token to a list of its sample_results.\n",
    "    :param class_name: Class to compute AP on.\n",
    "    :param dist_fcn: Distance function used to match detections and ground truths.\n",
    "    :param dist_th: Distance threshold for a match.\n",
    "    :param verbose: If true, print debug messages.\n",
    "    :return: (average_prec, metrics). The average precision value and raw data for a number of metrics.\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------\n",
    "    # Organize input and initialize accumulators.\n",
    "    # ---------------------------------------------\n",
    "    # Count the positives.\n",
    "    \n",
    "    # -- npos = This method is returning the number of ground truth annotations for a class \n",
    "    # -- -- gt_boxes is of type EvalBoxes, which groups nusc BBoxes (Translation, Orientation, etc) by sample\n",
    "    # -- -- all returns all bounding boxes\n",
    "    npos = len([1 for gt_box in gt_boxes.all if gt_box.detection_name == class_name])\n",
    "    if verbose:\n",
    "        print(\"Found {} GT of class {} out of {} total across {} samples.\".\n",
    "              format(npos, class_name, len(gt_boxes.all), len(gt_boxes.sample_tokens)))\n",
    "\n",
    "    # For missing classes in the GT, return a data structure corresponding to no predictions.\n",
    "    if npos == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # Organize the predictions in a single list.\n",
    "    pred_boxes_list = [box for box in pred_boxes.all if box.detection_name == class_name]\n",
    "    pred_confs = [box.detection_score for box in pred_boxes_list]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Found {} PRED of class {} out of {} total across {} samples.\".\n",
    "              format(len(pred_confs), class_name, len(pred_boxes.all), len(pred_boxes.sample_tokens)))\n",
    "\n",
    "    # Sort by confidence.\n",
    "    sortind = [i for (v, i) in sorted((v, i) for (i, v) in enumerate(pred_confs))][::-1]\n",
    "\n",
    "    # Do the actual matching.\n",
    "    tp = []  # Accumulator of true positives.\n",
    "    fp = []  # Accumulator of false positives.\n",
    "    conf = []  # Accumulator of confidences.\n",
    "\n",
    "    # match_data holds the extra metrics we calculate for each match.\n",
    "    match_data = {'trans_err': [],\n",
    "                  'vel_err': [],\n",
    "                  'scale_err': [],\n",
    "                  'orient_err': [],\n",
    "                  'attr_err': [],\n",
    "                  'conf': []}\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Match and accumulate match data.\n",
    "    # ---------------------------------------------\n",
    "    # -- Get the prediction with the greatest confidence\n",
    "    # -- -- pred_boxes_list is a list of BBoxes and match it with no gt\n",
    "    # -- -- for each ground truth bounding box\n",
    "    # -- -- -- if (the ground truth bounding box is belongs to the same class AND hasn't been assigned yet)\n",
    "    # -- -- -- -- consider this for a possible match. \n",
    "    #             Exit the loop with the a same class match that is the least distance away\n",
    "    # \n",
    "    taken = set()  # Initially no gt bounding box is matched.\n",
    "    for ind in sortind:\n",
    "        pred_box = pred_boxes_list[ind]\n",
    "        min_dist = np.inf\n",
    "        match_gt_idx = None\n",
    "\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes[pred_box.sample_token]):\n",
    "\n",
    "            # Find closest match among ground truth boxes\n",
    "            if gt_box.detection_name == class_name and not (pred_box.sample_token, gt_idx) in taken:\n",
    "                this_distance = dist_fcn(gt_box, pred_box)\n",
    "                if this_distance < min_dist:\n",
    "                    min_dist = this_distance\n",
    "                    match_gt_idx = gt_idx\n",
    "\n",
    "        # If the closest match is close enough according to threshold we have a match!\n",
    "        is_match = min_dist < dist_th\n",
    "\n",
    "        if is_match:\n",
    "            taken.add((pred_box.sample_token, match_gt_idx))\n",
    "\n",
    "            #  Update tp, fp and confs.\n",
    "            tp.append(1)\n",
    "            fp.append(0)\n",
    "            conf.append(pred_box.detection_score)\n",
    "\n",
    "            # Since it is a match, update match data also.\n",
    "            gt_box_match = gt_boxes[pred_box.sample_token][match_gt_idx]\n",
    "\n",
    "            match_data['trans_err'].append(center_distance(gt_box_match, pred_box))\n",
    "            match_data['vel_err'].append(velocity_l2(gt_box_match, pred_box))\n",
    "            match_data['scale_err'].append(1 - scale_iou(gt_box_match, pred_box))\n",
    "\n",
    "            # Barrier orientation is only determined up to 180 degree. (For cones orientation is discarded later)\n",
    "            period = np.pi if class_name == 'barrier' else 2 * np.pi\n",
    "            match_data['orient_err'].append(yaw_diff(gt_box_match, pred_box, period=period))\n",
    "\n",
    "            match_data['attr_err'].append(1 - attr_acc(gt_box_match, pred_box))\n",
    "            match_data['conf'].append(pred_box.detection_score)\n",
    "\n",
    "        else:\n",
    "            # No match. Mark this as a false positive.\n",
    "            tp.append(0)\n",
    "            fp.append(1)\n",
    "            conf.append(pred_box.detection_score)\n",
    "            \n",
    "    # Printing true positive rates of a class:\n",
    "    print(f\"The number of true positives out of total instances for class {class_name} for {dist_th}m \\n {len(tp)} out of {npos} \\n\")\n",
    "\n",
    "    # Check if we have any matches. If not, just return a \"no predictions\" array.\n",
    "    if len(match_data['trans_err']) == 0:\n",
    "        return DetectionMetricData.no_predictions()\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Calculate and interpolate precision and recall\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # Accumulate.\n",
    "    tp = np.cumsum(tp).astype(float)\n",
    "    fp = np.cumsum(fp).astype(float)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    # Calculate precision and recall.\n",
    "    prec = tp / (fp + tp)\n",
    "    rec = tp / float(npos)\n",
    "\n",
    "    rec_interp = np.linspace(0, 1, DetectionMetricData.nelem)  # 101 steps, from 0% to 100% recall.\n",
    "    prec = np.interp(rec_interp, rec, prec, right=0)\n",
    "    conf = np.interp(rec_interp, rec, conf, right=0)\n",
    "    rec = rec_interp\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # Re-sample the match-data to match, prec, recall and conf.\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    for key in match_data.keys():\n",
    "        if key == \"conf\":\n",
    "            continue  # Confidence is used as reference to align with fp and tp. So skip in this step.\n",
    "\n",
    "        else:\n",
    "            # For each match_data, we first calculate the accumulated mean.\n",
    "            tmp = cummean(np.array(match_data[key]))\n",
    "\n",
    "            # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n",
    "            match_data[key] = np.interp(conf[::-1], match_data['conf'][::-1], tmp[::-1])[::-1]\n",
    "\n",
    "    calculate_distance_param_conf_mat(gt_boxes, pred_boxes, sortind, class_name, tp, fp, conf, taken, match_data, conf_mat_mapping)\n",
    "    # ---------------------------------------------\n",
    "    # Done. Instantiate MetricData and return\n",
    "    # ---------------------------------------------\n",
    "    return DetectionMetricData(recall=rec,\n",
    "                               precision=prec,\n",
    "                               confidence=conf,\n",
    "                               trans_err=match_data['trans_err'],\n",
    "                               vel_err=match_data['vel_err'],\n",
    "                               scale_err=match_data['scale_err'],\n",
    "                               orient_err=match_data['orient_err'],\n",
    "                               attr_err=match_data['attr_err'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionEval:\n",
    "    \"\"\"\n",
    "    This is the official nuScenes detection evaluation code.\n",
    "    Results are written to the provided output_dir.\n",
    "\n",
    "    nuScenes uses the following detection metrics:\n",
    "    - Mean Average Precision (mAP): Uses center-distance as matching criterion; averaged over distance thresholds.\n",
    "    - True Positive (TP) metrics: Average of translation, velocity, scale, orientation and attribute errors.\n",
    "    - nuScenes Detection Score (NDS): The weighted sum of the above.\n",
    "\n",
    "    Here is an overview of the functions in this method:\n",
    "    - init: Loads GT annotations and predictions stored in JSON format and filters the boxes.\n",
    "    - run: Performs evaluation and dumps the metric data to disk.\n",
    "    - render: Renders various plots and dumps to disk.\n",
    "\n",
    "    We assume that:\n",
    "    - Every sample_token is given in the results, although there may be not predictions for that sample.\n",
    "\n",
    "    Please see https://www.nuscenes.org/object-detection for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nusc: NuScenes,\n",
    "                 config: DetectionConfig,\n",
    "                 result_path: str,\n",
    "                 eval_set: str,\n",
    "                 output_dir: str = None,\n",
    "                 verbose: bool = True,\n",
    "                 lower_thresh:float = -1, \n",
    "                 upper_thresh:float = np.inf):\n",
    "        \"\"\"\n",
    "        Initialize a DetectionEval object.\n",
    "        :param nusc: A NuScenes object.\n",
    "        :param config: A DetectionConfig object.\n",
    "        :param result_path: Path of the nuScenes JSON result file.\n",
    "        :param eval_set: The dataset split to evaluate on, e.g. train, val or test.\n",
    "        :param output_dir: Folder to save plots and results to.\n",
    "        :param verbose: Whether to print to stdout.\n",
    "        \"\"\"\n",
    "        self.nusc = nusc\n",
    "        self.result_path = result_path\n",
    "        self.eval_set = eval_set\n",
    "        self.output_dir = output_dir\n",
    "        self.verbose = verbose\n",
    "        self.cfg = config\n",
    "\n",
    "        # Check result file exists.\n",
    "        assert os.path.exists(result_path), 'Error: The result file does not exist!'\n",
    "\n",
    "        # Make dirs.\n",
    "        self.plot_dir = os.path.join(self.output_dir, 'plots')\n",
    "        if not os.path.isdir(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        if not os.path.isdir(self.plot_dir):\n",
    "            os.makedirs(self.plot_dir)\n",
    "\n",
    "        # Load data.\n",
    "        if verbose:\n",
    "            print('Initializing nuScenes detection evaluation')\n",
    "        self.pred_boxes, self.meta = load_prediction(self.result_path, self.cfg.max_boxes_per_sample, DetectionBox,\n",
    "                                                     verbose=verbose)\n",
    "        self.gt_boxes = load_gt(self.nusc, self.eval_set, DetectionBox, verbose=verbose)\n",
    "\n",
    "        assert set(self.pred_boxes.sample_tokens) == set(self.gt_boxes.sample_tokens), \\\n",
    "            \"Samples in split doesn't match samples in predictions.\"\n",
    "\n",
    "        # Add center distances.\n",
    "        self.pred_boxes = add_center_dist(nusc, self.pred_boxes)\n",
    "        self.gt_boxes = add_center_dist(nusc, self.gt_boxes)\n",
    "\n",
    "        # Filter boxes (distance, points per box, etc.).\n",
    "        if verbose:\n",
    "            print('Filtering predictions')\n",
    "        self.pred_boxes = filter_eval_boxes(nusc, self.pred_boxes, self.cfg.class_range, verbose=verbose)\n",
    "        if verbose:\n",
    "            print('Filtering ground truth annotations')\n",
    "        self.gt_boxes = filter_eval_boxes(nusc, self.gt_boxes, self.cfg.class_range, verbose=verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Removing samples outside of distance range')\n",
    "        \n",
    "        for gt in self.gt_boxes.all:\n",
    "            dist = np.sqrt(np.dot(gt.ego_translation, gt.ego_translation)) \n",
    "            if dist > upper_thresh or dist < lower_thresh:\n",
    "                self.gt_boxes.all.remove(gt)\n",
    "                self.gt_boxes.sample_tokens.remove(gt.sample_token)\n",
    "        \n",
    "        for pred in self.pred_boxes.all:\n",
    "            dist = np.sqrt(np.dot(pred.ego_translation, pred.ego_translation)) \n",
    "            if dist > upper_thresh or dist < lower_thresh:\n",
    "                self.pred_boxes.all.remove(pred)\n",
    "                self.pred_boxes.sample_tokens.remove(pred.sample_token)\n",
    "\n",
    "        self.sample_tokens = self.gt_boxes.sample_tokens\n",
    "        \n",
    "                \n",
    "        \n",
    "    def custom_evaluate(self) -> Tuple[DetectionMetrics, DetectionMetricDataList]:\n",
    "        \"\"\"\n",
    "        Performs the actual evaluation, but customized for metrics that are\n",
    "        of interest to us.\n",
    "        :return: A tuple of high-level and the raw metric data.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --  Changing to make this more consistent with how nuScenes calculates metrics\n",
    "        self.cfg.dist_ths = [2.0]\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 1: Accumulate metric data for all classes and distance thresholds.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Accumulating metric data...')\n",
    "        metric_data_list = DetectionMetricDataList()\n",
    "        \n",
    "        # -- For each class\n",
    "        # -- -- For each distance threshold of this class\n",
    "        # -- -- -- return the metric data for this class\n",
    "        for class_name in self.cfg.class_names:\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                print(self.cfg.dist_ths)\n",
    "                md = custom_accumulate(self.gt_boxes, self.pred_boxes, class_name, self.cfg.dist_fcn_callable, dist_th)\n",
    "                metric_data_list.set(class_name, dist_th, md)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Step 2: Calculate metrics from the data.\n",
    "        # -----------------------------------\n",
    "        if self.verbose:\n",
    "            print('Calculating metrics...')\n",
    "        metrics = DetectionMetrics(self.cfg)\n",
    "        for class_name in self.cfg.class_names:\n",
    "            # Compute APs.\n",
    "            for dist_th in self.cfg.dist_ths:\n",
    "                metric_data = metric_data_list[(class_name, dist_th)]\n",
    "                ap = calc_ap(metric_data, self.cfg.min_recall, self.cfg.min_precision)\n",
    "                metrics.add_label_ap(class_name, dist_th, ap)\n",
    "\n",
    "            # Compute TP metrics.\n",
    "            for metric_name in TP_METRICS:\n",
    "                metric_data = metric_data_list[(class_name, self.cfg.dist_th_tp)]\n",
    "                if class_name in ['traffic_cone'] and metric_name in ['attr_err', 'vel_err', 'orient_err']:\n",
    "                    tp = np.nan\n",
    "                elif class_name in ['barrier'] and metric_name in ['attr_err', 'vel_err']:\n",
    "                    tp = np.nan\n",
    "                else:\n",
    "                    tp = calc_tp(metric_data, self.cfg.min_recall, metric_name)\n",
    "                metrics.add_label_tp(class_name, metric_name, tp)\n",
    "\n",
    "        # Compute evaluation time.\n",
    "        metrics.add_runtime(time.time() - start_time)\n",
    "\n",
    "        return metrics, metric_data_list\n",
    "\n",
    "\n",
    "    def render(self, metrics: DetectionMetrics, md_list: DetectionMetricDataList) -> None:\n",
    "        \"\"\"\n",
    "        Renders various PR and TP curves.\n",
    "        :param metrics: DetectionMetrics instance.\n",
    "        :param md_list: DetectionMetricDataList instance.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print('Rendering PR and TP curves')\n",
    "\n",
    "        def savepath(name):\n",
    "            return os.path.join(self.plot_dir, name + '.pdf')\n",
    "\n",
    "        summary_plot(md_list, metrics, min_precision=self.cfg.min_precision, min_recall=self.cfg.min_recall,\n",
    "                     dist_th_tp=self.cfg.dist_th_tp, savepath=savepath('summary'))\n",
    "\n",
    "        for detection_name in self.cfg.class_names:\n",
    "            class_pr_curve(md_list, metrics, detection_name, self.cfg.min_precision, self.cfg.min_recall,\n",
    "                           savepath=savepath(detection_name + '_pr'))\n",
    "\n",
    "            class_tp_curve(md_list, metrics, detection_name, self.cfg.min_recall, self.cfg.dist_th_tp,\n",
    "                           savepath=savepath(detection_name + '_tp'))\n",
    "\n",
    "        for dist_th in self.cfg.dist_ths:\n",
    "            dist_pr_curve(md_list, metrics, dist_th, self.cfg.min_precision, self.cfg.min_recall,\n",
    "                          savepath=savepath('dist_pr_' + str(dist_th)))\n",
    "\n",
    "\n",
    "    def main(self,\n",
    "             plot_examples: int = 0,\n",
    "             render_curves: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main function that loads the evaluation code, visualizes samples, runs the evaluation and renders stat plots.\n",
    "        :param plot_examples: How many example visualizations to write to disk.\n",
    "        :param render_curves: Whether to render PR and TP curves to disk.\n",
    "        :return: A dict that stores the high-level metrics and meta data.\n",
    "        \"\"\"\n",
    "        if plot_examples > 0:\n",
    "            # Select a random but fixed subset to plot.\n",
    "            random.seed(42)\n",
    "            sample_tokens = list(self.sample_tokens)\n",
    "            random.shuffle(sample_tokens)\n",
    "            sample_tokens = sample_tokens[:plot_examples]\n",
    "\n",
    "            # Visualize samples.\n",
    "            example_dir = os.path.join(self.output_dir, 'examples')\n",
    "            if not os.path.isdir(example_dir):\n",
    "                os.mkdir(example_dir)\n",
    "            for sample_token in sample_tokens:\n",
    "                visualize_sample(self.nusc,\n",
    "                                 sample_token,\n",
    "                                 self.gt_boxes if self.eval_set != 'test' else EvalBoxes(),\n",
    "                                 # Don't render test GT.\n",
    "                                 self.pred_boxes,\n",
    "                                 eval_range=max(self.cfg.class_range.values()),\n",
    "                                 savepath=os.path.join(example_dir, '{}.png'.format(sample_token)))\n",
    "\n",
    "        # Run evaluation.\n",
    "        metrics, metric_data_list = self.evaluate()\n",
    "\n",
    "        # Render PR and TP curves.\n",
    "        if render_curves:\n",
    "            self.render(metrics, metric_data_list)\n",
    "\n",
    "        # Dump the metric data, meta and metrics to disk.\n",
    "        if self.verbose:\n",
    "            print('Saving metrics to: %s' % self.output_dir)\n",
    "        metrics_summary = metrics.serialize()\n",
    "        metrics_summary['meta'] = self.meta.copy()\n",
    "        with open(os.path.join(self.output_dir, 'metrics_summary.json'), 'w') as f:\n",
    "            json.dump(metrics_summary, f, indent=2)\n",
    "        with open(os.path.join(self.output_dir, 'metrics_details.json'), 'w') as f:\n",
    "            json.dump(metric_data_list.serialize(), f, indent=2)\n",
    "\n",
    "        # Print high-level metrics.\n",
    "        print('mAP: %.4f' % (metrics_summary['mean_ap']))\n",
    "        err_name_mapping = {\n",
    "            'trans_err': 'mATE',\n",
    "            'scale_err': 'mASE',\n",
    "            'orient_err': 'mAOE',\n",
    "            'vel_err': 'mAVE',\n",
    "            'attr_err': 'mAAE'\n",
    "        }\n",
    "        for tp_name, tp_val in metrics_summary['tp_errors'].items():\n",
    "            print('%s: %.4f' % (err_name_mapping[tp_name], tp_val))\n",
    "        print('NDS: %.4f' % (metrics_summary['nd_score']))\n",
    "        print('Eval time: %.1fs' % metrics_summary['eval_time'])\n",
    "\n",
    "        # Print per-class metrics.\n",
    "        print()\n",
    "        print('Per-class results:')\n",
    "        print('%-20s\\t%-6s\\t%-6s\\t%-6s\\t%-6s\\t%-6s\\t%-6s' % ('Object Class', 'AP', 'ATE', 'ASE', 'AOE', 'AVE', 'AAE'))\n",
    "        class_aps = metrics_summary['mean_dist_aps']\n",
    "        class_tps = metrics_summary['label_tp_errors']\n",
    "        for class_name in class_aps.keys():\n",
    "            print('%-20s\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f\\t%-6.3f'\n",
    "                % (class_name, class_aps[class_name],\n",
    "                    class_tps[class_name]['trans_err'],\n",
    "                    class_tps[class_name]['scale_err'],\n",
    "                    class_tps[class_name]['orient_err'],\n",
    "                    class_tps[class_name]['vel_err'],\n",
    "                    class_tps[class_name]['attr_err']))\n",
    "\n",
    "        return metrics_summary\n",
    "    \n",
    "    # def dist_calculation(self):\n",
    "    #     ego_poses = {}\n",
    "    #     random_gt = random.choice(self.gt_boxes.all)\n",
    "    #     random_ego = ego_poses[random_gt.sample_token]\n",
    "    #     print(f\" GT stored Distance = {random_gt.ego_translation}\")\n",
    "    #     print(f\" That as a distance is {np.sqrt(np.dot(random_gt.ego_translation, random_gt.ego_translation))}\")\n",
    "    #     print(f\" My Calculated distance is {center_distance(random_gt, random_ego)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing nuScenes detection evaluation\n",
      "Loaded results from /home/ranai/nuscenes_dataset/inference_results_mini/model2_good/results_nusc.json. Found detections for 81 samples.\n",
      "Loading annotations for mini_val split from nuScenes version: v1.0-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 271.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ground truth annotations for 81 samples.\n",
      "Filtering predictions\n",
      "=> Original number of boxes: 2040\n",
      "=> After distance based filtering: 2035\n",
      "=> After LIDAR and RADAR points based filtering: 2035\n",
      "=> After bike rack filtering: 2035\n",
      "Filtering ground truth annotations\n",
      "=> Original number of boxes: 4441\n",
      "=> After distance based filtering: 3785\n",
      "=> After LIDAR and RADAR points based filtering: 3393\n",
      "=> After bike rack filtering: 3393\n",
      "Removing samples outside of distance range\n"
     ]
    }
   ],
   "source": [
    "CustomDetectionEval = DetectionEval\n",
    "\n",
    "eval = CustomDetectionEval(\n",
    "    nusc=nusc,\n",
    "    config=eval_config,\n",
    "    result_path=f'{model_dir}/results_nusc.json',\n",
    "    eval_set=eval_set_map[dataset_version],\n",
    "    output_dir=os.getcwd(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating metric data...\n",
      "[2.0]\n",
      "The number of true positives out of total instances for class car for 2.0m \n",
      " 1478 out of 1913 \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m distance_param_conf_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(distance_param_conf_mat)\n",
      "Cell \u001b[0;32mIn[167], line 121\u001b[0m, in \u001b[0;36mDetectionEval.custom_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist_th \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdist_ths:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdist_ths)\n\u001b[0;32m--> 121\u001b[0m         md \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_accumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_fcn_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_th\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m         metric_data_list\u001b[38;5;241m.\u001b[39mset(class_name, dist_th, md)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate metrics from the data.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[166], line 155\u001b[0m, in \u001b[0;36mcustom_accumulate\u001b[0;34m(gt_boxes, pred_boxes, class_name, dist_fcn, dist_th, verbose)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m         match_data[key] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(conf[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], match_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], tmp[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 155\u001b[0m \u001b[43mcalculate_distance_param_conf_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msortind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_mat_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Done. Instantiate MetricData and return\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DetectionMetricData(recall\u001b[38;5;241m=\u001b[39mrec,\n\u001b[1;32m    160\u001b[0m                            precision\u001b[38;5;241m=\u001b[39mprec,\n\u001b[1;32m    161\u001b[0m                            confidence\u001b[38;5;241m=\u001b[39mconf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m                            orient_err\u001b[38;5;241m=\u001b[39mmatch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morient_err\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    166\u001b[0m                            attr_err\u001b[38;5;241m=\u001b[39mmatch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattr_err\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[161], line 35\u001b[0m, in \u001b[0;36mcalculate_distance_param_conf_mat\u001b[0;34m(gt_boxes, pred_boxes, sortind, class_name, tp, fp, conf, taken, match_data, conf_mat_mapping, dist_thresh, yaw_thresh)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_pred_len)):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m class_pred_len[i] \u001b[38;5;241m>\u001b[39m class_gt_len[i]:\n\u001b[0;32m---> 35\u001b[0m                 \u001b[43mdistance_param_conf_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[EMPTY] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m class_pred_len[i] \u001b[38;5;241m-\u001b[39m class_gt_len[i]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt \u001b[38;5;129;01min\u001b[39;00m sample_gt_list:\n\u001b[1;32m     39\u001b[0m         best_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m       \u001b[38;5;66;03m# Initialize best iou for a bbox with a value that cannot be achieved.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "distance_param_conf_mat = np.zeros((3, 3))\n",
    "eval.custom_evaluate()\n",
    "print(distance_param_conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q: Shouldn't the number of npos also change as dist_th changes?***\n",
    "\n",
    "    Seems like the number of false positives is a little inflated\n",
    "    Currently the code functions as such\n",
    "\n",
    "    ```\n",
    "    evaluate()\n",
    "        for each class\n",
    "            for one of 4 dist_thresholds in  [0.5, 1, 2, 4]m\n",
    "                def accumulate(gt, pred, dist thresh)\n",
    "                    if we find a successful match and it is less than the dist_thresh count that as tp\n",
    "                    else consider it a false positive\n",
    "                    NOTE: Look at the comments in that method before reading further\n",
    "    ```\n",
    "\n",
    "This is why you'll see that the values don't change for the different thresholds. I would think npos should change as we change what dist threshold to consider but it seems like it is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
